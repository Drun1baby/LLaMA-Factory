# å¾®è°ƒæ¨¡å‹æµ‹è¯• Web æ¡†æ¶

åŸºäº Flask çš„ LLaMA-Factory å¾®è°ƒæ¨¡å‹æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒå®æ—¶æµå¼è¾“å‡ºå’Œ llama.cpp åŠ é€Ÿã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- âœ… **Web ç•Œé¢**ï¼šç¾è§‚çš„èŠå¤©ç•Œé¢ï¼Œæ”¯æŒå®æ—¶å¯¹è¯
- âœ… **æµå¼è¾“å‡º**ï¼šè¾¹ç”Ÿæˆè¾¹æ˜¾ç¤ºï¼Œå®æ—¶æŸ¥çœ‹æ¨¡å‹è¾“å‡ºï¼ˆç±»ä¼¼ ChatGPTï¼‰
- âœ… **å‚æ•°è°ƒèŠ‚**ï¼šå¯è°ƒèŠ‚ Temperatureã€Top-Pã€æœ€å¤§ç”Ÿæˆé•¿åº¦
- âœ… **LoRA é€‚é…å™¨**ï¼šè‡ªåŠ¨åŠ è½½å¾®è°ƒçš„ LoRA æƒé‡
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨ bfloat16 ç²¾åº¦ï¼Œåˆå¹¶ LoRA æƒé‡
- ğŸš€ **llama.cpp æ”¯æŒ**ï¼šå¯é€‰çš„é«˜æ€§èƒ½ CPU æ¨ç†ï¼ˆé€Ÿåº¦æå‡ 5-10 å€ï¼‰

## ğŸ“ é¡¹ç›®ç»“æ„

```
model_test/
â”œâ”€â”€ run.py                          # åº”ç”¨å¯åŠ¨å…¥å£
â”œâ”€â”€ requirements.txt                # Python ä¾èµ–
â”œâ”€â”€ LLAMACPP_GUIDE.md              # llama.cpp é›†æˆæŒ‡å—
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py                 # Flask åº”ç”¨å·¥å‚
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # ä¸»é¡µè·¯ç”±
â”‚   â”‚   â””â”€â”€ api.py                  # API æ¥å£ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_service.py        # PyTorch æ¨¡å‹æœåŠ¡
â”‚   â”‚   â””â”€â”€ llamacpp_service.py     # llama.cpp æœåŠ¡ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ static/css/
â”‚   â”‚   â””â”€â”€ style.css               # å‰ç«¯æ ·å¼
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html              # å‰ç«¯é¡µé¢ï¼ˆæ”¯æŒæµå¼æ˜¾ç¤ºï¼‰
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py              # å·¥å…·å‡½æ•°
â””â”€â”€ logs/                           # æ—¥å¿—ç›®å½•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd /Users/drunkbaby/Desktop/Codes/AI/LLaMA-Factory/model_test
pip install -r requirements.txt
```

### 2. å¯åŠ¨æœåŠ¡

```bash
python run.py
```

### 3. è®¿é—® Web ç•Œé¢

æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼š`http://localhost:5000`

## ğŸ“¡ API æ¥å£

### æ™®é€šèŠå¤©æ¥å£

```bash
POST /api/chat
Content-Type: application/json

{
  "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
  "max_new_tokens": 256,
  "temperature": 0.7,
  "top_p": 0.9
}
```

### æµå¼èŠå¤©æ¥å£ï¼ˆæ¨èï¼‰

```bash
POST /api/chat_stream
Content-Type: application/json

{
  "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
  "max_new_tokens": 256,
  "temperature": 0.7,
  "top_p": 0.9
}

# å“åº”ï¼štext/event-stream (SSE)
data: {"token": "ä½ "}
data: {"token": "å¥½"}
data: {"token": "ï¼"}
...
data: {"done": true}
```

### å…¶ä»–æ¥å£

| æ¥å£ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/` | GET | Web ç•Œé¢ |
| `/api/load_model` | POST | æ‰‹åŠ¨åŠ è½½æ¨¡å‹ |
| `/api/model_status` | GET | æŸ¥çœ‹æ¨¡å‹çŠ¶æ€ |

## âš™ï¸ é…ç½®è¯´æ˜

ç¼–è¾‘ `config/settings.py` ä¿®æ”¹é…ç½®ï¼š

```python
MODEL_CONFIG = {
    "base_model_path": "tencent/Hunyuan-7B-Instruct",
    "lora_adapter_path": "../saves/Hunyuan-7B-Instruct/lora/save_Test",
    "device": "auto",  # auto/cuda/cpu/mps
    "quantization": None,  # None/4bit/8bit
    "max_new_tokens": 256,
    "temperature": 0.7,
    "top_p": 0.9,
}
```

## ğŸ¯ æµå¼è¾“å‡ºä½¿ç”¨

### Web ç•Œé¢

1. å‹¾é€‰"å¯ç”¨æµå¼è¾“å‡º"é€‰é¡¹
2. è¾“å…¥é—®é¢˜å¹¶å‘é€
3. æ¨¡å‹ä¼šè¾¹ç”Ÿæˆè¾¹æ˜¾ç¤ºï¼Œæ— éœ€ç­‰å¾…å…¨éƒ¨å®Œæˆ

### JavaScript ç¤ºä¾‹

```javascript
const eventSource = new EventSource('/api/chat_stream?' + params);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.token) {
        // æ˜¾ç¤ºç”Ÿæˆçš„ token
        console.log(data.token);
    }
    
    if (data.done) {
        // ç”Ÿæˆå®Œæˆ
        eventSource.close();
    }
};
```

### Python ç¤ºä¾‹

```python
import requests

response = requests.post(
    'http://localhost:5000/api/chat_stream',
    json={'prompt': 'ä½ å¥½'},
    stream=True
)

for line in response.iter_lines():
    if line.startswith(b'data: '):
        data = json.loads(line[6:])
        if 'token' in data:
            print(data['token'], end='', flush=True)
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### å½“å‰ä¼˜åŒ–ï¼ˆå·²åº”ç”¨ï¼‰

| ä¼˜åŒ–é¡¹ | æ•ˆæœ |
|--------|------|
| bfloat16 ç²¾åº¦ | é€Ÿåº¦æå‡ 2-3 å€ï¼Œå†…å­˜å‡åŠ |
| åˆå¹¶ LoRA æƒé‡ | é€Ÿåº¦æå‡ 10-20% |
| æµå¼è¾“å‡º | é¦– token å»¶è¿Ÿé™ä½ï¼Œç”¨æˆ·ä½“éªŒæå‡ |
| å‡å°‘é»˜è®¤ç”Ÿæˆé•¿åº¦ | å“åº”æ—¶é—´å‡åŠ |

### è¿›é˜¶ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

#### 1. ä½¿ç”¨ llama.cppï¼ˆæ¨èï¼‰

é€Ÿåº¦æå‡ **5-10 å€**ï¼Œè¯¦è§ [LLAMACPP_GUIDE.md](LLAMACPP_GUIDE.md)

```bash
# å®‰è£…
pip install llama-cpp-python

# è½¬æ¢æ¨¡å‹ä¸º GGUF æ ¼å¼
# å‚è€ƒ LLAMACPP_GUIDE.md
```

#### 2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

- Qwen2.5-1.5B-Instructï¼ˆé€Ÿåº¦å¿« 5 å€ï¼‰
- Phi-3-mini-4k-instructï¼ˆé€Ÿåº¦å¿« 3 å€ï¼‰

#### 3. äº‘ç«¯ GPU éƒ¨ç½²

- Google Colabï¼ˆå…è´¹ T4 GPUï¼‰
- Kaggle Notebooksï¼ˆå…è´¹ P100 GPUï¼‰
- é€Ÿåº¦æå‡ï¼š50-100 å€

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

åœ¨ Apple M2 Max (12 æ ¸ CPU, 32GB å†…å­˜) ä¸Šçš„æµ‹è¯•ï¼š

| æ–¹æ¡ˆ | ç”Ÿæˆé€Ÿåº¦ | å†…å­˜å ç”¨ | é¦– token å»¶è¿Ÿ |
|------|---------|---------|--------------|
| åŸå§‹ float32 | ~5 tokens/s | 28GB | 3-5 ç§’ |
| **bfloat16 + åˆå¹¶ LoRA** | ~12 tokens/s | 14GB | 2-3 ç§’ |
| **+ æµå¼è¾“å‡º** | ~12 tokens/s | 14GB | **0.5 ç§’** |
| llama.cpp (q4_0) | ~45 tokens/s | 4.5GB | 0.3 ç§’ |

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡Œ**ä¼šè‡ªåŠ¨ä¸‹è½½åŸºç¡€æ¨¡å‹ï¼ˆçº¦ 14GBï¼‰
2. **å†…å­˜éœ€æ±‚**ï¼šè‡³å°‘ 16GB ç³»ç»Ÿå†…å­˜ï¼ˆæ¨è 32GBï¼‰
3. **Mac MPS é™åˆ¶**ï¼š7B æ¨¡å‹è¶…å‡º MPS å†…å­˜é™åˆ¶ï¼Œè‡ªåŠ¨ä½¿ç”¨ CPU
4. **æµå¼è¾“å‡º**ï¼šéœ€è¦æµè§ˆå™¨æ”¯æŒ EventSourceï¼ˆç°ä»£æµè§ˆå™¨å‡æ”¯æŒï¼‰

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šæ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥ LoRA è·¯å¾„æ˜¯å¦æ­£ç¡®
ls -la ../saves/Hunyuan-7B-Instruct/lora/save_Test
```

### é—®é¢˜ 2ï¼šé€Ÿåº¦å¤ªæ…¢

- ç¡®è®¤å·²å¯ç”¨ bfloat16ï¼ˆæŸ¥çœ‹æ—¥å¿—ï¼‰
- å‡å°‘ `max_new_tokens` å‚æ•°
- è€ƒè™‘ä½¿ç”¨ llama.cpp

### é—®é¢˜ 3ï¼šæµå¼è¾“å‡ºä¸å·¥ä½œ

- æ£€æŸ¥æµè§ˆå™¨æ§åˆ¶å°æ˜¯å¦æœ‰é”™è¯¯
- ç¡®è®¤ Flask ç‰ˆæœ¬ >= 2.3.0
- å°è¯•å…³é—­æµè§ˆå™¨çš„å¹¿å‘Šæ‹¦æˆªæ’ä»¶

### é—®é¢˜ 4ï¼šå†…å­˜ä¸è¶³

```python
# åœ¨ settings.py ä¸­å¯ç”¨é‡åŒ–ï¼ˆéœ€è¦ CUDAï¼‰
MODEL_CONFIG = {
    "quantization": "4bit",  # æˆ– "8bit"
    ...
}
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct)
- [llama.cpp é›†æˆæŒ‡å—](LLAMACPP_GUIDE.md)
- [Flask æ–‡æ¡£](https://flask.palletsprojects.com/)
- [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.1.0 (2025-12-05)

- âœ¨ æ–°å¢æµå¼è¾“å‡ºæ”¯æŒï¼ˆSSEï¼‰
- âœ¨ æ–°å¢ llama.cpp é›†æˆï¼ˆå¯é€‰ï¼‰
- ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šbfloat16 + åˆå¹¶ LoRA
- ğŸ“ å®Œå–„æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

### v1.0.0 (2025-12-05)

- ğŸ‰ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… åŸºç¡€ Web ç•Œé¢
- âœ… LoRA æ¨¡å‹åŠ è½½
- âœ… å‚æ•°è°ƒèŠ‚åŠŸèƒ½

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
